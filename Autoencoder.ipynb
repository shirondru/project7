{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c796a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from numpy.typing import ArrayLike\n",
    "from nn import NeuralNetwork, one_hot_encode_seqs, sample_seqs\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e782a0",
   "metadata": {},
   "source": [
    "### 5-fold CV Grid Search to find optimal NN architecture\n",
    "* 5-fold Grid search CV was implemented manually to find optimal architecture (i.e, # of layers) hyperparameter \n",
    "* Data was split into train and test set. Test set was completely held out for the grid search CV; grid search hyperparameter tuning was done using only the training data. \n",
    "* Note that the grid search will take >1hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "316ff326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,56) and (54,48) not aligned: 56 (dim 1) != 54 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/pb/rxdm6429115bqbpvsxy37dbm0000gn/T/ipykernel_20616/1342036210.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     68\u001B[0m                         loss_function='mean_squared_error') \n\u001B[1;32m     69\u001B[0m         \u001B[0;31m#fit this fold. test data is still held out\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 70\u001B[0;31m         train_loss, val_loss = nn.fit(X_train[train],y_train[train],\n\u001B[0m\u001B[1;32m     71\u001B[0m                                        X_train[val],y_train[val])\n\u001B[1;32m     72\u001B[0m         \u001B[0mfold_avg_train_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loss\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#avg (across epochs) training loss for this fold\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/GitHub/project7/nn/nn.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X_train, y_train, X_val, y_val)\u001B[0m\n\u001B[1;32m    330\u001B[0m             \u001B[0mbatch_val_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    331\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mX_batch_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_batch_train\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_batch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 332\u001B[0;31m                 \u001B[0my_hat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcache\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_batch_train\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#get prediction with training data from this batch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    333\u001B[0m                 \u001B[0mbatch_train_loss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_loss_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_batch_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0my_hat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#save training loss from this batch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m                 \u001B[0mgrad_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackprop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_batch_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0my_hat\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcache\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#get partial derivative of loss with respect to each weight and bias term\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/GitHub/project7/nn/nn.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    146\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m                 \u001B[0mA_prev\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mA_curr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 148\u001B[0;31m             A_curr, Z_curr = self._single_forward(W_curr=self._param_dict['W' + str(layer_idx)],\n\u001B[0m\u001B[1;32m    149\u001B[0m                                                   \u001B[0mb_curr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_param_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'b'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    150\u001B[0m                                                   \u001B[0mA_prev\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mA_prev\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/GitHub/project7/nn/nn.py\u001B[0m in \u001B[0;36m_single_forward\u001B[0;34m(self, W_curr, b_curr, A_prev, activation)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 118\u001B[0;31m         \u001B[0mZ_curr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mA_prev\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mW_curr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mb_curr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    119\u001B[0m         \u001B[0mA_curr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_activation_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mZ_curr\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mactivation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: shapes (1,56) and (54,48) not aligned: 56 (dim 1) != 54 (dim 0)"
     ]
    }
   ],
   "source": [
    "## load and split data into train and test set. test set will be completely held out\n",
    "# of grid search via cross validation.\n",
    "np.random.seed(42)\n",
    "digits = datasets.load_digits()\n",
    "X = digits['data']\n",
    "y = digits['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# perform Grid search via cross validation on the training data  \n",
    "#training data will be split into train and validation set, while the original \n",
    "#test set is held out\n",
    "\n",
    "#saving the average MSE across all epochs in each fold with each combination of proposed\n",
    "#hyperparameters\n",
    "columns = [\"size\",\"avg_train_error\",\"avg_val_error\"] + [f\"train_error_fold_{x}\" for x in range(1,6)] \\\n",
    "         + [f\"val_error_fold_{x}\" for x in range(1,6)]            \n",
    "tuning_results_df = pd.DataFrame(columns = columns)\n",
    "\n",
    "\n",
    "sizes = {2:\n",
    "       [{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'},  \n",
    "       {'input_dim': 16, 'output_dim': 64, 'activation': 'relu'}],\n",
    "    \n",
    "       4:\n",
    "        [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'},\n",
    "       {'input_dim': 32, 'output_dim': 16, 'activation': 'relu'},\n",
    "       {'input_dim': 16, 'output_dim': 32, 'activation': 'relu'},  \n",
    "       {'input_dim': 32, 'output_dim': 64, 'activation': 'relu'}],\n",
    "         \n",
    "         6:\n",
    "          [{'input_dim': 64, 'output_dim': 48, 'activation': 'relu'},\n",
    "       {'input_dim': 48, 'output_dim': 32, 'activation': 'relu'},\n",
    "       {'input_dim': 32, 'output_dim': 16, 'activation': 'relu'},\n",
    "       {'input_dim': 16, 'output_dim': 32, 'activation': 'relu'},\n",
    "       {'input_dim': 32, 'output_dim': 48, 'activation': 'relu'},\n",
    "       {'input_dim': 48, 'output_dim': 64, 'activation': 'relu'}],\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "df_idx = 0\n",
    "for size in sizes:\n",
    "    print(size)\n",
    "    fold_avg_train_losses = []\n",
    "    fold_avg_val_losses = []\n",
    "    for fold_idx, (train,val) in enumerate(kf.split(X_train,y_train)):\n",
    "        fold_idx = fold_idx + 1\n",
    "        \n",
    "        \n",
    "        # fit NN using current architecture size (i.e, 2 layers, 4 layers, etc)\n",
    "        # using stochastic gradient descent\n",
    "        nn = NeuralNetwork(nn_arch=sizes[size],\n",
    "                       lr=0.001, batch_size=1, seed=42, epochs=500, \n",
    "                        loss_function='mean_squared_error') \n",
    "        #fit this fold. test data is still held out\n",
    "        train_loss, val_loss = nn.fit(X_train[train],y_train[train],\n",
    "                                       X_train[val],y_train[val])\n",
    "        fold_avg_train_loss = np.mean(train_loss) #avg (across epochs) training loss for this fold\n",
    "        fold_avg_train_losses.append(fold_avg_train_loss)\n",
    "\n",
    "        fold_avg_val_loss = np.mean(val_loss) #avg (across epochs) validation loss for this fold\n",
    "        fold_avg_val_losses.append(fold_avg_val_loss)\n",
    "\n",
    "\n",
    "        #store hyperparameters and per-fold results\n",
    "        tuning_results_df.loc[df_idx,\"size\"]=size\n",
    "        tuning_results_df.loc[df_idx,f\"train_error_fold_{fold_idx}\"] = fold_avg_train_loss\n",
    "        tuning_results_df.loc[df_idx,f\"val_error_fold_{fold_idx}\"] = fold_avg_val_loss\n",
    "\n",
    "    #store average train and val loss across all folds for this combination of hyperparameters\n",
    "    tuning_results_df.loc[df_idx,\"avg_train_error\"] = np.mean(fold_avg_train_losses)\n",
    "    tuning_results_df.loc[df_idx,\"avg_val_error\"] = np.mean(fold_avg_val_losses)\n",
    "\n",
    "    df_idx +=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f3b8709a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>avg_train_error</th>\n",
       "      <th>avg_val_error</th>\n",
       "      <th>train_error_fold_1</th>\n",
       "      <th>train_error_fold_2</th>\n",
       "      <th>train_error_fold_3</th>\n",
       "      <th>train_error_fold_4</th>\n",
       "      <th>train_error_fold_5</th>\n",
       "      <th>val_error_fold_1</th>\n",
       "      <th>val_error_fold_2</th>\n",
       "      <th>val_error_fold_3</th>\n",
       "      <th>val_error_fold_4</th>\n",
       "      <th>val_error_fold_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>8.170597</td>\n",
       "      <td>8.163216</td>\n",
       "      <td>7.594967</td>\n",
       "      <td>7.922113</td>\n",
       "      <td>8.018097</td>\n",
       "      <td>8.547432</td>\n",
       "      <td>8.770379</td>\n",
       "      <td>7.120172</td>\n",
       "      <td>8.093121</td>\n",
       "      <td>8.465553</td>\n",
       "      <td>8.63345</td>\n",
       "      <td>8.503786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>6.080416</td>\n",
       "      <td>6.10391</td>\n",
       "      <td>4.266077</td>\n",
       "      <td>6.291071</td>\n",
       "      <td>6.826829</td>\n",
       "      <td>6.26392</td>\n",
       "      <td>6.754185</td>\n",
       "      <td>3.987092</td>\n",
       "      <td>6.256604</td>\n",
       "      <td>7.224299</td>\n",
       "      <td>6.380818</td>\n",
       "      <td>6.670737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>7.868176</td>\n",
       "      <td>7.865956</td>\n",
       "      <td>6.009164</td>\n",
       "      <td>6.859886</td>\n",
       "      <td>6.232084</td>\n",
       "      <td>13.899059</td>\n",
       "      <td>6.340684</td>\n",
       "      <td>5.626828</td>\n",
       "      <td>6.896004</td>\n",
       "      <td>6.602483</td>\n",
       "      <td>13.911804</td>\n",
       "      <td>6.292663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.072019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.221626</td>\n",
       "      <td>5.847786</td>\n",
       "      <td>14.012945</td>\n",
       "      <td>13.215926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.844092</td>\n",
       "      <td>5.970574</td>\n",
       "      <td>13.25842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  size avg_train_error avg_val_error train_error_fold_1 train_error_fold_2  \\\n",
       "0    2        8.170597      8.163216           7.594967           7.922113   \n",
       "1    4        6.080416       6.10391           4.266077           6.291071   \n",
       "2    6        7.868176      7.865956           6.009164           6.859886   \n",
       "3    8             NaN           NaN          14.072019                NaN   \n",
       "\n",
       "  train_error_fold_3 train_error_fold_4 train_error_fold_5 val_error_fold_1  \\\n",
       "0           8.018097           8.547432           8.770379         7.120172   \n",
       "1           6.826829            6.26392           6.754185         3.987092   \n",
       "2           6.232084          13.899059           6.340684         5.626828   \n",
       "3          12.221626           5.847786          14.012945        13.215926   \n",
       "\n",
       "  val_error_fold_2 val_error_fold_3 val_error_fold_4 val_error_fold_5  \n",
       "0         8.093121         8.465553          8.63345         8.503786  \n",
       "1         6.256604         7.224299         6.380818         6.670737  \n",
       "2         6.896004         6.602483        13.911804         6.292663  \n",
       "3              NaN        12.844092         5.970574         13.25842  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#resulting dataframe stores error from each fold with each combination of hyperparameters\n",
    "#as well as the average error across all 5 folds with each combnation of hyperparameters\n",
    "display(tuning_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "03c4b730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size                         4\n",
       "avg_train_error       6.080416\n",
       "avg_val_error          6.10391\n",
       "train_error_fold_1    4.266077\n",
       "train_error_fold_2    6.291071\n",
       "train_error_fold_3    6.826829\n",
       "train_error_fold_4     6.26392\n",
       "train_error_fold_5    6.754185\n",
       "val_error_fold_1      3.987092\n",
       "val_error_fold_2      6.256604\n",
       "val_error_fold_3      7.224299\n",
       "val_error_fold_4      6.380818\n",
       "val_error_fold_5      6.670737\n",
       "Name: 1, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal neural network had 4 layers\n"
     ]
    }
   ],
   "source": [
    "#get data for combination of hyperparameters that gave the lowest average MSE\n",
    "# across epochs and folds in the validation set\n",
    "display(tuning_results_df.loc[tuning_results_df['avg_val_error'].astype(float).idxmin()])\n",
    "best_nn_size = tuning_results_df.loc[tuning_results_df['avg_val_error'].astype(float).idxmin(),'size']\n",
    "\n",
    "print(f\"The optimal neural network had {best_nn_size} layers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299df244",
   "metadata": {},
   "source": [
    "#### Re-training model with optimal set of hyperparameters and plotting per-epoch train and test MSE\n",
    "\n",
    "* I tuned the architecture (i.e # of layers) hyperparameter of the neural network by implementing a cross-validated grid search; the size of the NN that minimized the average across-fold MSE was used moving forward. A subset of samples were completely held out from this procedure to be used for validation/testing here. \n",
    "* These results suggest the model was able to learn from the training data, as the training loss decreased as the number of epochs increased. After about 2 epochs, the validation loss began to plateau, suggesting the model was beginning to overfit to the training data after this point,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "631ab202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn = NeuralNetwork(nn_arch = sizes[best_nn_size],\n",
    "                       lr=0.001, batch_size=1, seed=42, epochs=500, \n",
    "                        loss_function='mean_squared_error')\n",
    "\n",
    "#X_train and y_train are the same observations that were used for cross validation grid search\n",
    "#X_test and y_test were completely held out of the grid search procedure\n",
    "per_epoch_train_loss, per_epoch_val_loss = nn.fit(X_train,y_train,\n",
    "                                               X_test,y_test)\n",
    "\n",
    "\n",
    "\n",
    "loss_hist = per_epoch_train_loss\n",
    "loss_hist_val = per_epoch_val_loss\n",
    "assert len(loss_hist) > 0, \"Need to run training before plotting loss history\"\n",
    "fig, axs = plt.subplots(2, figsize=(8,8))\n",
    "fig.suptitle('Loss History')\n",
    "axs[0].plot(np.arange(len(loss_hist)), loss_hist)\n",
    "axs[0].set_title('Training Loss')\n",
    "axs[1].plot(np.arange(len(loss_hist_val)), loss_hist_val)\n",
    "axs[1].set_title('Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "axs[0].set_ylabel('Train Loss')\n",
    "axs[1].set_ylabel('Val Loss')\n",
    "fig.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e1ef6",
   "metadata": {},
   "source": [
    "### Average reconstruction error over held out test dataset\n",
    "* Take trained model and form predictions on entire datase then calculate MSE across the held out test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "62330b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reconstruction error over unseen test data 29.955334595959595\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#average MSE from unseen test data\n",
    "test_predictions = nn.predict(X_test)\n",
    "test_reconstruction_error = nn._mean_squared_error(X_test,test_predictions)\n",
    "print(f\"Average reconstruction error over unseen test data {test_reconstruction_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ab18dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.array([[1,1,1],\n",
    "                  [2,2,2],\n",
    "                  [3,3,3]])\n",
    "\n",
    "test= NeuralNetwork(nn_arch=[{'input_dim': 3, 'output_dim': 2, 'activation': 'relu'},\n",
    "                        {'input_dim': 2, 'output_dim': 3, 'activation': 'relu'}],\n",
    "                       lr=0.001, batch_size=3, seed=42, epochs=25,\n",
    "                       loss_function='mean_squared_error')\n",
    "\n",
    "test._param_dict = {'W1': np.array([[ 0.5, 0.5,  0.5],\n",
    "                    [ 0.5, 0.5, 0.5 ]]),\n",
    "             'b1': np.array([[1.0],\n",
    "                            [1.0]]),\n",
    "             'W2': np.array([[0.5,  0.5 ],\n",
    "                    [0.5, 0.5],\n",
    "                    [ 0.5, 0.5]]),\n",
    "             'b2': np.array([[1.0],\n",
    "                            [1.0],\n",
    "                            [1.0]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6a2029c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output,cache = test.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d52ce922",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_curr = cache[\"Z2\"]\n",
    "dZdW = cache[\"A1\"]\n",
    "dAdZ = test._activation_function_backprop(Z_curr, 'relu')\n",
    "dJdA = dJdA = (output - X) / X.size\n",
    "delta = np.multiply(dJdA, dAdZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1a1ecff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dW_curr = delta.T.dot(dZdW)\n",
    "db_curr = np.sum(delta,axis = 0,keepdims=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "017e5538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27777778, 0.27777778, 0.27777778],\n",
       "       [0.33333333, 0.33333333, 0.33333333],\n",
       "       [0.38888889, 0.38888889, 0.38888889]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "41360c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.16666667, 4.16666667],\n",
       "       [4.16666667, 4.16666667],\n",
       "       [4.16666667, 4.16666667]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d7e988c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(dW_curr,dW_curr) and np.allclose(dW_curr,dW_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4655fccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1af4e455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.5, 12.5],\n",
       "       [12.5, 12.5],\n",
       "       [12.5, 12.5]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2e7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}